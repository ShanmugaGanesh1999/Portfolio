# Fault Tolerance

> Building systems that continue operating despite failures

---

## ðŸ“– What is Fault Tolerance?

**Fault Tolerance** is a system's ability to continue functioning correctly even when some of its components fail.

```
Fault Tolerant System:
Component fails â†’ System continues â†’ User unaffected

Non-Fault Tolerant System:
Component fails â†’ System crashes â†’ User impacted
```

---

## ðŸŽ¯ Faults vs Failures

| Term | Definition | Example |
|------|------------|---------|
| **Fault** | A component not working correctly | Server disk dies |
| **Error** | Wrong behavior caused by fault | Can't read file |
| **Failure** | System not providing expected service | Website down |

**Goal**: Prevent faults from becoming failures.

```
Fault â”€â”€â–º Error â”€â”€â–º Failure
  â”‚                    â”‚
  â””â”€â”€ Fault Tolerance â”€â”˜
       (breaks the chain)
```

---

## ðŸ”§ Fault Tolerance Techniques

### 1. Redundancy

Have backups for critical components.

**Hardware Redundancy**:
```
Single Point of Failure:
[Single Server] â”€â”€â–º If it dies, system dies

Redundant:
[Server 1] â†â”€â”€â”
              â”œâ”€â”€ Load Balancer â”€â”€â–º Users
[Server 2] â†â”€â”€â”˜
              If one dies, other continues
```

**Data Redundancy**:
```
Primary Database â”€â”€â–º Replica 1
        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Replica 2
        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Replica 3

Data on multiple nodes
Survive N-1 failures
```

**Geographic Redundancy**:
```
US-East Region â—„â”€â”€â”€â”€â”€â”€â–º US-West Region
     â”‚                        â”‚
  [Servers]               [Servers]
  [Database] â—„â”€syncâ”€â”€â–º [Database]

Survive entire data center failure
```

### 2. Replication

Keep multiple copies of data.

**Synchronous Replication**:
```
Write â”€â”€â–º Primary â”€â”€â–º Wait for replicas â”€â”€â–º ACK
               â”‚
          â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
          â–¼         â–¼
       Replica 1  Replica 2

Pros: No data loss
Cons: Higher latency
```

**Asynchronous Replication**:
```
Write â”€â”€â–º Primary â”€â”€â–º ACK (immediate)
               â”‚
               â””â”€â”€â”€â–º Background replication
                         â”‚
                    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                    â–¼         â–¼
                 Replica 1  Replica 2

Pros: Lower latency
Cons: Possible data loss on failure
```

### 3. Failover

Automatically switch to backup on failure.

**Active-Passive Failover**:
```
Normal:
Primary â”€â”€â–º Traffic
Standby â”€â”€â–º (idle, receiving replication)

After Primary Failure:
Primary â”€â”€X (dead)
Standby â”€â”€â–º Traffic (promoted to primary)
```

**Active-Active Failover**:
```
Normal:
Server A â”€â”€â–º 50% Traffic
Server B â”€â”€â–º 50% Traffic

After A Failure:
Server A â”€â”€X (dead)
Server B â”€â”€â–º 100% Traffic
```

### 4. Graceful Degradation

Provide reduced functionality instead of total failure.

```python
def get_product_page(product_id):
    # Core functionality - must work
    product = get_product(product_id)  # From database
    
    # Non-critical - can fail gracefully
    try:
        recommendations = get_recommendations(product_id)
    except ServiceUnavailable:
        recommendations = []  # Show page without recommendations
    
    try:
        reviews = get_reviews(product_id)
    except ServiceUnavailable:
        reviews = None  # Show "Reviews temporarily unavailable"
    
    return render_page(product, recommendations, reviews)
```

### 5. Circuit Breaker

Stop calling failing services.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CIRCUIT BREAKER                          â”‚
â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚  CLOSED  â”‚â”€â”€â”€â”€â–¶â”‚   OPEN   â”‚â”€â”€â”€â”€â–¶â”‚HALF-OPEN â”‚           â”‚
â”‚   â”‚          â”‚     â”‚          â”‚     â”‚          â”‚           â”‚
â”‚   â”‚ Normal   â”‚     â”‚ Fail fastâ”‚     â”‚  Test    â”‚           â”‚
â”‚   â”‚ operationâ”‚     â”‚ (no calls)â”‚    â”‚ a call   â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜           â”‚
â”‚        â”‚                â”‚                â”‚                  â”‚
â”‚        â”‚                â”‚                â”‚                  â”‚
â”‚   failures >       timeout           success?               â”‚
â”‚   threshold        expires              â”‚                   â”‚
â”‚                         â”‚         â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”            â”‚
â”‚                         â”‚         â”‚           â”‚            â”‚
â”‚                         â–¼         â–¼           â–¼            â”‚
â”‚                      CLOSED    CLOSED       OPEN           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
class CircuitBreaker:
    def __init__(self, failure_threshold=5, recovery_timeout=30):
        self.failures = 0
        self.state = "CLOSED"
        self.last_failure_time = None
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
    
    def call(self, func):
        if self.state == "OPEN":
            if time.time() - self.last_failure_time > self.recovery_timeout:
                self.state = "HALF_OPEN"
            else:
                raise CircuitOpenError()
        
        try:
            result = func()
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise
    
    def _on_success(self):
        self.failures = 0
        self.state = "CLOSED"
    
    def _on_failure(self):
        self.failures += 1
        self.last_failure_time = time.time()
        if self.failures >= self.failure_threshold:
            self.state = "OPEN"
```

### 6. Bulkhead Pattern

Isolate failures to prevent cascade.

```
Without Bulkhead:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Shared Thread Pool         â”‚
â”‚  [Service A calls] [Service B calls]â”‚
â”‚                                    â”‚
â”‚  If B is slow, all threads blocked â”‚
â”‚  Service A calls also fail!        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

With Bulkhead:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pool for A  â”‚  â”‚  Pool for B  â”‚
â”‚  [A calls]   â”‚  â”‚  [B calls]   â”‚
â”‚              â”‚  â”‚              â”‚
â”‚  If B slow,  â”‚  â”‚  Only B pool â”‚
â”‚  A unaffectedâ”‚  â”‚  is blocked  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7. Timeouts and Retries

Don't wait forever; retry intelligently.

```python
def call_with_retry(func, max_retries=3, base_delay=1):
    """Retry with exponential backoff and jitter"""
    for attempt in range(max_retries):
        try:
            return func()
        except TransientError as e:
            if attempt == max_retries - 1:
                raise
            
            # Exponential backoff with jitter
            delay = base_delay * (2 ** attempt)
            jitter = random.uniform(0, delay * 0.1)
            time.sleep(delay + jitter)
    
    raise MaxRetriesExceeded()
```

**Timeout Best Practices**:
```
Service A â”€(timeout: 5s)â”€â–º Service B â”€(timeout: 3s)â”€â–º Service C

Each layer's timeout should be less than the caller's
to allow for error handling and retries
```

---

## ðŸ“Š Fault Tolerance Patterns Summary

| Pattern | Use When | Trade-off |
|---------|----------|-----------|
| **Redundancy** | Component failure is common | Cost (more resources) |
| **Replication** | Data loss is unacceptable | Complexity, latency |
| **Failover** | Need automatic recovery | Failover delay |
| **Graceful Degradation** | Some features can fail | Reduced functionality |
| **Circuit Breaker** | Dependent services fail | Temporarily unavailable |
| **Bulkhead** | Isolate failure domains | Resource partitioning |
| **Timeout/Retry** | Transient failures occur | Increased latency |

---

## ðŸŽ¨ Real-World Example: Netflix

Netflix's fault tolerance strategy:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Netflix Architecture                      â”‚
â”‚                                                              â”‚
â”‚  Hystrix (Circuit Breaker)                                  â”‚
â”‚  â”œâ”€â”€ Every service call wrapped                             â”‚
â”‚  â”œâ”€â”€ Fast failure when downstream is slow                   â”‚
â”‚  â””â”€â”€ Fallbacks for degraded experience                      â”‚
â”‚                                                              â”‚
â”‚  Chaos Monkey (Fault Injection)                             â”‚
â”‚  â”œâ”€â”€ Randomly kills production instances                    â”‚
â”‚  â”œâ”€â”€ Forces teams to build resilient systems                â”‚
â”‚  â””â”€â”€ Failures become non-events                             â”‚
â”‚                                                              â”‚
â”‚  Multi-Region Deployment                                    â”‚
â”‚  â”œâ”€â”€ Active in multiple AWS regions                         â”‚
â”‚  â”œâ”€â”€ Traffic shifts if region has issues                    â”‚
â”‚  â””â”€â”€ Can evacuate entire regions                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”„ Designing for Fault Tolerance

### Questions to Ask

1. **What can fail?**
   - Hardware, network, software, humans

2. **What's the impact of each failure?**
   - Critical path vs nice-to-have

3. **How do we detect failures?**
   - Health checks, monitoring, alerts

4. **How do we recover?**
   - Automatic failover, manual intervention

5. **How do we prevent cascade?**
   - Isolation, circuit breakers

### Design Process

```
1. Identify failure modes
   â””â”€â–º List all components that can fail

2. Assess impact
   â””â”€â–º Rank by severity and likelihood

3. Design mitigation
   â””â”€â–º Apply appropriate patterns

4. Test failures
   â””â”€â–º Chaos engineering, fault injection

5. Monitor and iterate
   â””â”€â–º Track incidents, improve
```

---

## ðŸ§ª Testing Fault Tolerance

### Chaos Engineering

Deliberately inject failures to test resilience.

```
Chaos Engineering Steps:
1. Define steady state (normal behavior)
2. Hypothesize: system will remain stable
3. Inject failure (kill server, network latency, etc.)
4. Observe behavior
5. Fix if hypothesis is wrong
```

### Types of Fault Injection

| Type | Example | Tests |
|------|---------|-------|
| **Instance** | Kill random server | Auto-scaling, failover |
| **Network** | Add latency, drop packets | Timeout handling |
| **Dependency** | Make database slow | Circuit breakers |
| **Resource** | Fill disk, exhaust memory | Resource limits |
| **Time** | Clock skew | Time-sensitive logic |

---

## âš ï¸ Common Mistakes

### 1. Retry Storm

```
Problem:
Service down â†’ All clients retry â†’ Massive spike
              â†’ Service overloaded â†’ Stays down

Solution:
â”œâ”€â”€ Exponential backoff
â”œâ”€â”€ Jitter (random delay)
â”œâ”€â”€ Circuit breaker
â””â”€â”€ Retry budget (max retries per time window)
```

### 2. Ignoring Partial Failures

```
Problem:
3 out of 5 DB writes succeed
Transaction left in inconsistent state

Solution:
â”œâ”€â”€ Transactions (all or nothing)
â”œâ”€â”€ Sagas with compensating actions
â””â”€â”€ Idempotent operations
```

### 3. Single Points of Failure

```
Hidden SPOFs:
â”œâ”€â”€ "Highly available" database with single master
â”œâ”€â”€ Load balancer (who balances the balancer?)
â”œâ”€â”€ Configuration server
â”œâ”€â”€ DNS
â””â”€â”€ The one person who knows the system
```

---

## ðŸ’¡ Interview Tips

### When Discussing Fault Tolerance

1. **Identify failure points**
   - "What happens if this component fails?"

2. **Explain mitigation**
   - "We use replication for data redundancy"
   - "Circuit breakers prevent cascade"

3. **Discuss trade-offs**
   - Consistency vs availability
   - Complexity vs resilience

4. **Mention monitoring**
   - "We detect failures through health checks"
   - "Alerts trigger automatic failover"

---

## âœ… Key Takeaways

1. **Failures are inevitable** - Design for them
2. **Redundancy is key** - No single points of failure
3. **Detect fast** - Quick detection enables quick recovery
4. **Fail gracefully** - Degraded service > no service
5. **Isolate failures** - Prevent cascade with bulkheads
6. **Test failures** - Chaos engineering in production
7. **Automate recovery** - Manual intervention is slow

---

## ðŸ“– Next Steps

â†’ Continue to [Estimation](../03-Estimation/README.md)
