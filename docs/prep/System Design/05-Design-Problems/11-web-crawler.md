# Design Web Crawler

> Distributed system to crawl and index the web

---

## ğŸ“‹ Problem Statement

Design a web crawler that systematically browses the web to collect pages for a search engine, similar to Googlebot.

---

## R - Requirements

### Functional Requirements

```
1. Crawl billions of web pages
2. Handle various content types (HTML, PDF, images)
3. Respect robots.txt and crawl politeness
4. Detect and avoid duplicate content
5. Handle dynamic/JavaScript content
6. Prioritize important pages
```

### Non-Functional Requirements

```
1. Scalable to crawl entire web
2. Efficient (minimize redundant crawls)
3. Polite (don't overwhelm servers)
4. Fresh (re-crawl frequently updated pages)
5. Robust (handle failures gracefully)
```

---

## E - Estimation

```
Scale:
â”œâ”€â”€ 1 billion pages to crawl
â”œâ”€â”€ Average page size: 100KB (HTML + assets)
â”œâ”€â”€ Re-crawl every 2 weeks on average
â”œâ”€â”€ Total crawl rate: 1B / 14 days = 850 pages/sec

Storage:
â”œâ”€â”€ Raw pages: 1B Ã— 100KB = 100TB
â”œâ”€â”€ Processed data: ~50TB
â”œâ”€â”€ URL frontier: 10B URLs Ã— 100 bytes = 1TB

Bandwidth:
â”œâ”€â”€ 850 pages/sec Ã— 100KB = 85 MB/sec
â”œâ”€â”€ ~7 TB/day download
```

---

## H - High-Level Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                  URL Frontier                        â”‚   â”‚
â”‚   â”‚              (Priority Queue + Politeness)           â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                  Crawler Workers                     â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”          â”‚   â”‚
â”‚   â”‚   â”‚Worker1â”‚ â”‚Worker2â”‚ â”‚Worker3â”‚ â”‚WorkerNâ”‚          â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜          â”‚   â”‚
â”‚   â”‚       â”‚         â”‚         â”‚         â”‚                â”‚   â”‚
â”‚   â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â”‚                                         â”‚
â”‚                    â–¼                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚   â”‚           Content Processor             â”‚                â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                â”‚
â”‚   â”‚  â”‚ Parse HTML â†’ Extract URLs â†’ Store â”‚  â”‚                â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                    â”‚                                         â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚        â–¼           â–¼           â–¼                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚   â”‚  URL    â”‚ â”‚ Content â”‚ â”‚ Search   â”‚                      â”‚
â”‚   â”‚ Filter  â”‚ â”‚  Store  â”‚ â”‚  Index   â”‚                      â”‚
â”‚   â”‚(Dedup)  â”‚ â”‚  (S3)   â”‚ â”‚  (ES)    â”‚                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## D - Detailed Design

### URL Frontier

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              URL Frontier                                    â”‚
â”‚                                                              â”‚
â”‚   Two main concerns:                                         â”‚
â”‚   1. Prioritization: Which URLs to crawl first             â”‚
â”‚   2. Politeness: Don't overwhelm any single server         â”‚
â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚               Priority Queues                        â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚   â”‚  â”‚  High   â”‚ â”‚ Medium  â”‚ â”‚   Low   â”‚ â”‚  Retry  â”‚   â”‚   â”‚
â”‚   â”‚  â”‚Priority â”‚ â”‚Priority â”‚ â”‚Priority â”‚ â”‚  Queue  â”‚   â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚   â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚
â”‚   â”‚                   â–¼                                  â”‚   â”‚
â”‚   â”‚            Queue Selector                            â”‚   â”‚
â”‚   â”‚        (Weighted random selection)                   â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚             Per-Host Queues (Politeness)             â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚   â”‚
â”‚   â”‚  â”‚ host1   â”‚ â”‚ host2   â”‚ â”‚ host3   â”‚ ...           â”‚   â”‚
â”‚   â”‚  â”‚.com     â”‚ â”‚.org     â”‚ â”‚.net     â”‚               â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚   â”‚
â”‚   â”‚                                                      â”‚   â”‚
â”‚   â”‚  Rate limiter: 1 request per host every 1-2 seconds â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Priority Calculation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              URL Priority                                    â”‚
â”‚                                                              â”‚
â”‚   Factors:                                                   â”‚
â”‚   1. PageRank of linking pages                              â”‚
â”‚   2. Freshness (when last crawled)                         â”‚
â”‚   3. Update frequency (how often it changes)               â”‚
â”‚   4. Depth from seed URLs                                  â”‚
â”‚                                                              â”‚
â”‚   Priority = w1 Ã— pagerank +                                â”‚
â”‚              w2 Ã— freshness_score +                         â”‚
â”‚              w3 Ã— update_frequency +                        â”‚
â”‚              w4 Ã— (1 / depth)                               â”‚
â”‚                                                              â”‚
â”‚   Examples:                                                  â”‚
â”‚   - cnn.com homepage â†’ High priority (changes often)       â”‚
â”‚   - Personal blog from 2015 â†’ Low priority                 â”‚
â”‚   - Linked from many pages â†’ Higher priority               â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Crawler Worker

```python
class CrawlerWorker:
    def __init__(self):
        self.session = aiohttp.ClientSession()
        self.robots_cache = {}
    
    async def crawl(self, url):
        # 1. Check robots.txt
        if not await self.is_allowed(url):
            return None
        
        # 2. Fetch page
        try:
            async with self.session.get(url, timeout=10) as resp:
                if resp.status != 200:
                    return None
                content = await resp.text()
                content_type = resp.headers.get('Content-Type')
        except Exception as e:
            # Add to retry queue
            return None
        
        # 3. Parse and extract
        result = {
            'url': url,
            'content': content,
            'content_type': content_type,
            'links': self.extract_links(content, url),
            'crawled_at': time.time()
        }
        
        return result
    
    async def is_allowed(self, url):
        host = urlparse(url).netloc
        if host not in self.robots_cache:
            robots_url = f"https://{host}/robots.txt"
            # Fetch and parse robots.txt
            self.robots_cache[host] = await self.fetch_robots(robots_url)
        
        return self.robots_cache[host].is_allowed(url)
    
    def extract_links(self, html, base_url):
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        for a in soup.find_all('a', href=True):
            absolute_url = urljoin(base_url, a['href'])
            links.append(absolute_url)
        return links
```

### URL Deduplication

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Deduplication                                   â”‚
â”‚                                                              â”‚
â”‚   Problem: Avoid re-crawling same URL / same content        â”‚
â”‚                                                              â”‚
â”‚   1. URL-level dedup:                                       â”‚
â”‚      â”œâ”€â”€ Normalize URLs (lowercase, remove fragments)      â”‚
â”‚      â”œâ”€â”€ Store URL hash in Bloom filter                    â”‚
â”‚      â””â”€â”€ Check before adding to frontier                   â”‚
â”‚                                                              â”‚
â”‚   Bloom Filter:                                              â”‚
â”‚   â”œâ”€â”€ Space-efficient probabilistic set                    â”‚
â”‚   â”œâ”€â”€ 10B URLs in ~10GB with 1% false positive            â”‚
â”‚   â”œâ”€â”€ Check: O(k) hash lookups                             â”‚
â”‚   â””â”€â”€ False positives OK (skip some URLs)                  â”‚
â”‚                                                              â”‚
â”‚   2. Content-level dedup:                                   â”‚
â”‚      â”œâ”€â”€ Compute content hash (SimHash for fuzzy)          â”‚
â”‚      â”œâ”€â”€ Detect near-duplicate pages                       â”‚
â”‚      â””â”€â”€ Store canonical URL                               â”‚
â”‚                                                              â”‚
â”‚   SimHash:                                                   â”‚
â”‚   â”œâ”€â”€ Content â†’ 64-bit fingerprint                         â”‚
â”‚   â”œâ”€â”€ Similar pages have similar hashes                    â”‚
â”‚   â”œâ”€â”€ Compare Hamming distance                             â”‚
â”‚   â””â”€â”€ Distance < 3 = likely duplicate                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Robots.txt Handling

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Robots.txt                                      â”‚
â”‚                                                              â”‚
â”‚   Example robots.txt:                                        â”‚
â”‚   User-agent: *                                              â”‚
â”‚   Disallow: /private/                                        â”‚
â”‚   Disallow: /temp/                                           â”‚
â”‚   Crawl-delay: 2                                             â”‚
â”‚                                                              â”‚
â”‚   User-agent: Googlebot                                      â”‚
â”‚   Allow: /                                                   â”‚
â”‚                                                              â”‚
â”‚   Sitemap: https://example.com/sitemap.xml                  â”‚
â”‚                                                              â”‚
â”‚   Implementation:                                            â”‚
â”‚   1. Fetch robots.txt before crawling domain               â”‚
â”‚   2. Cache for 24 hours                                    â”‚
â”‚   3. Parse rules for our user-agent                        â”‚
â”‚   4. Check each URL against rules                          â”‚
â”‚   5. Respect Crawl-delay                                   â”‚
â”‚                                                              â”‚
â”‚   Politeness beyond robots.txt:                             â”‚
â”‚   â”œâ”€â”€ Limit concurrent requests per domain: 1              â”‚
â”‚   â”œâ”€â”€ Add delay between requests: 1-2 seconds              â”‚
â”‚   â”œâ”€â”€ Reduce during peak hours                             â”‚
â”‚   â””â”€â”€ Monitor 429/503 responses                            â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Content Processing Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Processing Pipeline                             â”‚
â”‚                                                              â”‚
â”‚   Raw Page                                                   â”‚
â”‚       â”‚                                                      â”‚
â”‚       â–¼                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚   â”‚  Content Parser   â”‚ â† HTML, PDF, JS rendering          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚             â”‚                                                â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                         â”‚
â”‚       â–¼           â–¼                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚   â”‚ Links â”‚   â”‚  Text  â”‚                                    â”‚
â”‚   â””â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                    â”‚
â”‚       â”‚           â”‚                                          â”‚
â”‚       â–¼           â–¼                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚   â”‚ URL   â”‚   â”‚  Content   â”‚                                â”‚
â”‚   â”‚Filter â”‚   â”‚  Analysis  â”‚                                â”‚
â”‚   â””â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚       â”‚             â”‚                                        â”‚
â”‚       â–¼             â–¼                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚   â”‚Add to â”‚   â”‚ Store in   â”‚                                â”‚
â”‚   â”‚Frontierâ”‚  â”‚  Index     â”‚                                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                              â”‚
â”‚   Content analysis:                                          â”‚
â”‚   â”œâ”€â”€ Language detection                                    â”‚
â”‚   â”œâ”€â”€ Spam/quality scoring                                 â”‚
â”‚   â”œâ”€â”€ Entity extraction                                    â”‚
â”‚   â””â”€â”€ Category classification                              â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Handling JavaScript

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              JavaScript Rendering                            â”‚
â”‚                                                              â”‚
â”‚   Problem: Many sites use JavaScript to render content       â”‚
â”‚                                                              â”‚
â”‚   Solution: Headless browser rendering                       â”‚
â”‚                                                              â”‚
â”‚   Architecture:                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚   â”‚        Rendering Service            â”‚                   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚                   â”‚
â”‚   â”‚  â”‚   Headless Chrome Pool      â”‚    â”‚                   â”‚
â”‚   â”‚  â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”      â”‚    â”‚                   â”‚
â”‚   â”‚  â”‚  â”‚Tab1â”‚ â”‚Tab2â”‚ â”‚Tab3â”‚ ...  â”‚    â”‚                   â”‚
â”‚   â”‚  â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜      â”‚    â”‚                   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                              â”‚
â”‚   Process:                                                   â”‚
â”‚   1. Load URL in headless Chrome                            â”‚
â”‚   2. Wait for page load + JavaScript execution              â”‚
â”‚   3. Extract rendered HTML                                  â”‚
â”‚   4. Much slower (seconds vs milliseconds)                  â”‚
â”‚                                                              â”‚
â”‚   Strategy:                                                  â”‚
â”‚   â”œâ”€â”€ Try simple fetch first                               â”‚
â”‚   â”œâ”€â”€ Detect if JS-rendered (empty body, frameworks)       â”‚
â”‚   â”œâ”€â”€ Use rendering service only when needed               â”‚
â”‚   â””â”€â”€ Cache rendered results                               â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Summary

```
Key Components:
â”œâ”€â”€ URL Frontier: Priority + politeness queues
â”œâ”€â”€ Crawler Workers: Async HTTP fetchers
â”œâ”€â”€ Deduplication: Bloom filter + SimHash
â”œâ”€â”€ Content Processor: Parse, extract, store

Key Decisions:
â”œâ”€â”€ Per-host queues for politeness
â”œâ”€â”€ Bloom filter for URL dedup (space-efficient)
â”œâ”€â”€ Priority based on PageRank + freshness
â”œâ”€â”€ Headless rendering for JS sites

Scale:
â”œâ”€â”€ 1000+ pages/second
â”œâ”€â”€ Billions of URLs tracked
â”œâ”€â”€ Petabytes of content stored
â”œâ”€â”€ Distributed across data centers
```

---

## ğŸ“– Next Steps

â†’ Continue to [Design Google Docs](./12-google-docs.md)
