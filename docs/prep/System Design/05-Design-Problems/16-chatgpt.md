# Design ChatGPT

> Large Language Model serving with streaming responses

---

## ðŸ“‹ Problem Statement

Design a system like ChatGPT that serves large language models to millions of users with conversational AI capabilities.

---

## R - Requirements

### Functional Requirements

```
1. Accept user prompts and generate responses
2. Maintain conversation history (context)
3. Stream responses token-by-token
4. Support multiple models (GPT-3.5, GPT-4)
5. Handle system prompts and personas
6. Plugin/tool integration (code execution, search)
7. Rate limiting per user/tier
```

### Non-Functional Requirements

```
1. Low time-to-first-token (<500ms)
2. Handle millions of concurrent users
3. High availability (99.9%)
4. Efficient GPU utilization
5. Cost-effective inference
```

---

## E - Estimation

```
Users: 100M weekly active
Conversations: 50M/day
Tokens per conversation: 2000 (input + output)

Compute:
â”œâ”€â”€ 50M Ã— 2000 tokens = 100B tokens/day
â”œâ”€â”€ 1.15M tokens/second average
â”œâ”€â”€ Peak: 3M tokens/second

GPU requirements:
â”œâ”€â”€ GPT-3.5: ~7B params, needs 14GB VRAM (FP16)
â”œâ”€â”€ GPT-4: ~1.8T params (rumored), multi-GPU
â”œâ”€â”€ Throughput: ~100 tokens/sec per GPU
â”œâ”€â”€ Need: 30,000+ GPUs for peak load

Storage:
â”œâ”€â”€ Conversation history: 50M Ã— 10KB = 500GB/day
â”œâ”€â”€ Model weights: 100GB-2TB per model
â”œâ”€â”€ User data: 100M Ã— 1KB = 100GB
```

---

## H - High-Level Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                  API Gateway                         â”‚   â”‚
â”‚   â”‚           (Auth, Rate Limiting, Routing)            â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚               Request Orchestrator                   â”‚   â”‚
â”‚   â”‚          (Context assembly, Tool dispatch)          â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚               Inference Queue                        â”‚   â”‚
â”‚   â”‚             (Priority, Batching)                    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚              GPU Inference Cluster                   â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚   â”‚
â”‚   â”‚  â”‚GPU Nodeâ”‚ â”‚GPU Nodeâ”‚ â”‚GPU Nodeâ”‚ â”‚GPU Nodeâ”‚ ...   â”‚   â”‚
â”‚   â”‚  â”‚(vLLM)  â”‚ â”‚(vLLM)  â”‚ â”‚(vLLM)  â”‚ â”‚(vLLM)  â”‚       â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                   Data Layer                          â”‚  â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚   â”‚  â”‚  Redis  â”‚  â”‚PostgreSQLâ”‚  â”‚   Blob Storage    â”‚   â”‚  â”‚
â”‚   â”‚  â”‚(Session)â”‚  â”‚  (Users) â”‚  â”‚  (Model Weights)  â”‚   â”‚  â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## D - Detailed Design

### LLM Inference Basics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              How LLM Inference Works                         â”‚
â”‚                                                              â”‚
â”‚   Autoregressive generation:                                 â”‚
â”‚   - Model generates one token at a time                     â”‚
â”‚   - Each new token depends on all previous tokens           â”‚
â”‚   - Cannot parallelize token generation                     â”‚
â”‚                                                              â”‚
â”‚   Example:                                                   â”‚
â”‚   Input: "What is the capital of France?"                   â”‚
â”‚   Output generation:                                         â”‚
â”‚   Step 1: "The" (attend to input)                          â”‚
â”‚   Step 2: " capital" (attend to input + "The")              â”‚
â”‚   Step 3: " of" (attend to input + "The capital")           â”‚
â”‚   Step 4: " France" (attend to all previous)               â”‚
â”‚   Step 5: " is" ...                                        â”‚
â”‚   Step 6: " Paris" ...                                     â”‚
â”‚   Step 7: "." [STOP]                                        â”‚
â”‚                                                              â”‚
â”‚   Two phases:                                                â”‚
â”‚   1. Prefill: Process input tokens (parallelizable)        â”‚
â”‚   2. Decode: Generate output tokens (sequential)           â”‚
â”‚                                                              â”‚
â”‚   Bottleneck: Memory bandwidth (loading weights per token) â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Streaming Response

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Streaming Architecture                          â”‚
â”‚                                                              â”‚
â”‚   Why stream?                                                â”‚
â”‚   - User sees response immediately                          â”‚
â”‚   - Better UX than waiting 30 seconds                       â”‚
â”‚   - Time-to-first-token is key metric                       â”‚
â”‚                                                              â”‚
â”‚   Implementation: Server-Sent Events (SSE)                   â”‚
â”‚                                                              â”‚
â”‚   Client request:                                            â”‚
â”‚   POST /v1/chat/completions                                  â”‚
â”‚   {"messages": [...], "stream": true}                       â”‚
â”‚                                                              â”‚
â”‚   Server response (chunked):                                 â”‚
â”‚   ```                                                       â”‚
â”‚   data: {"choices": [{"delta": {"content": "The"}}]}       â”‚
â”‚   data: {"choices": [{"delta": {"content": " capital"}}]}  â”‚
â”‚   data: {"choices": [{"delta": {"content": " is"}}]}       â”‚
â”‚   data: {"choices": [{"delta": {"content": " Paris"}}]}    â”‚
â”‚   data: {"choices": [{"delta": {"content": "."}}]}         â”‚
â”‚   data: [DONE]                                              â”‚
â”‚   ```                                                       â”‚
â”‚                                                              â”‚
â”‚   Client:                                                    â”‚
â”‚   ```javascript                                             â”‚
â”‚   const response = await fetch('/v1/chat/completions', {   â”‚
â”‚     method: 'POST',                                         â”‚
â”‚     body: JSON.stringify({messages, stream: true})         â”‚
â”‚   });                                                        â”‚
â”‚                                                              â”‚
â”‚   const reader = response.body.getReader();                 â”‚
â”‚   while (true) {                                            â”‚
â”‚     const {done, value} = await reader.read();             â”‚
â”‚     if (done) break;                                        â”‚
â”‚     // Parse and display token                             â”‚
â”‚   }                                                          â”‚
â”‚   ```                                                       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### KV Cache and Optimization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              KV Cache                                        â”‚
â”‚                                                              â”‚
â”‚   Problem: Each new token needs to attend to all previous   â”‚
â”‚   Naive: Recompute attention for all tokens each step       â”‚
â”‚   Solution: Cache Key-Value pairs from previous tokens      â”‚
â”‚                                                              â”‚
â”‚   Memory usage per request:                                  â”‚
â”‚   KV cache size = 2 Ã— layers Ã— heads Ã— head_dim Ã— seq_len  â”‚
â”‚   GPT-3: 2 Ã— 96 Ã— 96 Ã— 128 Ã— 4096 = ~10GB per request!     â”‚
â”‚                                                              â”‚
â”‚   Optimizations:                                             â”‚
â”‚                                                              â”‚
â”‚   1. PagedAttention (vLLM):                                 â”‚
â”‚      â”œâ”€â”€ Manage KV cache like OS virtual memory            â”‚
â”‚      â”œâ”€â”€ Allocate fixed-size blocks                        â”‚
â”‚      â”œâ”€â”€ Non-contiguous storage                            â”‚
â”‚      â””â”€â”€ 24Ã— higher throughput                             â”‚
â”‚                                                              â”‚
â”‚   2. Continuous batching:                                   â”‚
â”‚      â”œâ”€â”€ Don't wait for batch to complete                  â”‚
â”‚      â”œâ”€â”€ Add new requests as old ones finish               â”‚
â”‚      â”œâ”€â”€ Maximizes GPU utilization                         â”‚
â”‚      â””â”€â”€ Reduces queuing time                              â”‚
â”‚                                                              â”‚
â”‚   3. Speculative decoding:                                  â”‚
â”‚      â”œâ”€â”€ Small model drafts tokens                         â”‚
â”‚      â”œâ”€â”€ Large model verifies in parallel                  â”‚
â”‚      â”œâ”€â”€ Accept correct tokens, reject others              â”‚
â”‚      â””â”€â”€ 2-3Ã— speedup                                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Request Batching

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Batching Strategy                               â”‚
â”‚                                                              â”‚
â”‚   Naive batching:                                            â”‚
â”‚   - Wait for N requests, process together                  â”‚
â”‚   - Problem: All wait for slowest (longest output)         â”‚
â”‚                                                              â”‚
â”‚   Continuous batching:                                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ Time â†’                                               â”‚   â”‚
â”‚   â”‚ Request A: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (long response)         â”‚   â”‚
â”‚   â”‚ Request B: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (medium)                        â”‚   â”‚
â”‚   â”‚ Request C: â–ˆâ–ˆâ–ˆâ–ˆ (short)                             â”‚   â”‚
â”‚   â”‚ Request D:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (arrives mid-batch)     â”‚   â”‚
â”‚   â”‚ Request E:         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚   - C finishes first â†’ slot freed for D                    â”‚
â”‚   - B finishes â†’ slot freed for E                          â”‚
â”‚   - A continues uninterrupted                              â”‚
â”‚   - Result: Much higher throughput                         â”‚
â”‚                                                              â”‚
â”‚   Implementation:                                            â”‚
â”‚   - Inference engine (vLLM) handles batching               â”‚
â”‚   - Request queue feeds engine                             â”‚
â”‚   - Priority queues for paid tiers                         â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Conversation Context

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Context Management                              â”‚
â”‚                                                              â”‚
â”‚   Conversation format:                                       â”‚
â”‚   {                                                          â”‚
â”‚     "messages": [                                           â”‚
â”‚       {"role": "system", "content": "You are helpful..."},  â”‚
â”‚       {"role": "user", "content": "Hello!"},                â”‚
â”‚       {"role": "assistant", "content": "Hi there!"},        â”‚
â”‚       {"role": "user", "content": "What's 2+2?"}            â”‚
â”‚     ]                                                        â”‚
â”‚   }                                                          â”‚
â”‚                                                              â”‚
â”‚   Context window limits:                                     â”‚
â”‚   â”œâ”€â”€ GPT-3.5-turbo: 16K tokens                            â”‚
â”‚   â”œâ”€â”€ GPT-4: 8K/32K/128K tokens                            â”‚
â”‚   â””â”€â”€ Must truncate or summarize if exceeds                â”‚
â”‚                                                              â”‚
â”‚   Truncation strategies:                                     â”‚
â”‚   1. Sliding window: Keep last N tokens                    â”‚
â”‚   2. Summarization: Compress old messages                  â”‚
â”‚   3. Selective: Keep system + recent + important           â”‚
â”‚                                                              â”‚
â”‚   Session storage:                                           â”‚
â”‚   â”œâ”€â”€ Redis for active sessions (fast access)             â”‚
â”‚   â”œâ”€â”€ PostgreSQL for persistence                          â”‚
â”‚   â”œâ”€â”€ TTL on inactive sessions                            â”‚
â”‚   â””â”€â”€ User can load history on new session                â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tool/Plugin Integration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Function Calling / Tools                        â”‚
â”‚                                                              â”‚
â”‚   Enable LLM to use external tools:                         â”‚
â”‚                                                              â”‚
â”‚   Available tools:                                           â”‚
â”‚   â”œâ”€â”€ Web search                                           â”‚
â”‚   â”œâ”€â”€ Code interpreter (Python sandbox)                   â”‚
â”‚   â”œâ”€â”€ Image generation (DALL-E)                            â”‚
â”‚   â”œâ”€â”€ Database queries                                     â”‚
â”‚   â””â”€â”€ Custom APIs                                          â”‚
â”‚                                                              â”‚
â”‚   Flow:                                                      â”‚
â”‚   1. User: "What's the weather in Tokyo?"                  â”‚
â”‚   2. LLM generates function call:                          â”‚
â”‚      {"name": "get_weather", "args": {"city": "Tokyo"}}    â”‚
â”‚   3. Orchestrator executes function                        â”‚
â”‚   4. Result injected into context                          â”‚
â”‚   5. LLM generates final response                          â”‚
â”‚                                                              â”‚
â”‚   Tool definition:                                           â”‚
â”‚   {                                                          â”‚
â”‚     "name": "get_weather",                                  â”‚
â”‚     "description": "Get current weather for a city",        â”‚
â”‚     "parameters": {                                         â”‚
â”‚       "type": "object",                                     â”‚
â”‚       "properties": {                                       â”‚
â”‚         "city": {"type": "string", "description": "City"}  â”‚
â”‚       },                                                     â”‚
â”‚       "required": ["city"]                                  â”‚
â”‚     }                                                        â”‚
â”‚   }                                                          â”‚
â”‚                                                              â”‚
â”‚   Security:                                                  â”‚
â”‚   â”œâ”€â”€ Sandboxed code execution                             â”‚
â”‚   â”œâ”€â”€ Rate limits on tools                                 â”‚
â”‚   â”œâ”€â”€ Validate tool outputs                                â”‚
â”‚   â””â”€â”€ Audit logging                                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Serving Infrastructure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              GPU Cluster Architecture                        â”‚
â”‚                                                              â”‚
â”‚   Inference stack:                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Application Layer (FastAPI)                         â”‚   â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚   â”‚  Inference Engine (vLLM, TensorRT-LLM, TGI)         â”‚   â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚   â”‚  ML Framework (PyTorch, JAX)                        â”‚   â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚   â”‚  CUDA / cuDNN                                        â”‚   â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚   â”‚  GPU Hardware (A100, H100)                          â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚   Scaling:                                                   â”‚
â”‚   â”œâ”€â”€ Horizontal: More GPU nodes                           â”‚
â”‚   â”œâ”€â”€ Tensor parallelism: Split model across GPUs         â”‚
â”‚   â”œâ”€â”€ Pipeline parallelism: Split layers across GPUs      â”‚
â”‚   â””â”€â”€ Auto-scaling based on queue depth                   â”‚
â”‚                                                              â”‚
â”‚   Model deployment:                                          â”‚
â”‚   â”œâ”€â”€ Model weights in blob storage (S3)                  â”‚
â”‚   â”œâ”€â”€ Load on node startup                                â”‚
â”‚   â”œâ”€â”€ NVMe cache for fast loading                         â”‚
â”‚   â””â”€â”€ Version management                                  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Rate Limiting and Tiers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Rate Limiting                                   â”‚
â”‚                                                              â”‚
â”‚   Tiers:                                                     â”‚
â”‚   â”œâ”€â”€ Free: 10 requests/min, 3.5 only, short context       â”‚
â”‚   â”œâ”€â”€ Plus: 50 requests/min, GPT-4 access                  â”‚
â”‚   â”œâ”€â”€ API: Token-based billing                             â”‚
â”‚   â””â”€â”€ Enterprise: Custom limits                            â”‚
â”‚                                                              â”‚
â”‚   Limits enforced at:                                        â”‚
â”‚   â”œâ”€â”€ Requests per minute                                  â”‚
â”‚   â”œâ”€â”€ Tokens per minute                                    â”‚
â”‚   â”œâ”€â”€ Tokens per day                                       â”‚
â”‚   â””â”€â”€ Concurrent requests                                  â”‚
â”‚                                                              â”‚
â”‚   Implementation:                                            â”‚
â”‚   â”œâ”€â”€ Redis token bucket                                   â”‚
â”‚   â”œâ”€â”€ Sliding window counters                              â”‚
â”‚   â”œâ”€â”€ API key â†’ tier mapping                              â”‚
â”‚   â””â”€â”€ Return 429 with Retry-After header                  â”‚
â”‚                                                              â”‚
â”‚   Priority queuing:                                          â”‚
â”‚   â”œâ”€â”€ Paid users get higher priority                       â”‚
â”‚   â”œâ”€â”€ Separate queues per tier                            â”‚
â”‚   â”œâ”€â”€ Fair scheduling within tier                         â”‚
â”‚   â””â”€â”€ Preemption for high-priority                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“Š API Design

```
# Chat completions (OpenAI-compatible)
POST /v1/chat/completions
{
    "model": "gpt-4",
    "messages": [
        {"role": "system", "content": "You are helpful."},
        {"role": "user", "content": "Hello!"}
    ],
    "stream": true,
    "max_tokens": 1000,
    "temperature": 0.7
}

# Response (streamed)
data: {"id": "chatcmpl-xxx", "choices": [{"delta": {"content": "Hi"}}]}
data: {"id": "chatcmpl-xxx", "choices": [{"delta": {"content": "!"}}]}
data: [DONE]

# Non-streamed response
{
    "id": "chatcmpl-xxx",
    "choices": [{
        "message": {"role": "assistant", "content": "Hi!"},
        "finish_reason": "stop"
    }],
    "usage": {"prompt_tokens": 10, "completion_tokens": 50}
}
```

---

## ðŸ“Š Summary

```
Key Components:
â”œâ”€â”€ API Gateway: Auth, rate limiting, routing
â”œâ”€â”€ Request Orchestrator: Context, tools, streaming
â”œâ”€â”€ Inference Queue: Batching, priority
â”œâ”€â”€ GPU Cluster: vLLM/TGI for efficient inference

Key Optimizations:
â”œâ”€â”€ PagedAttention for memory efficiency
â”œâ”€â”€ Continuous batching for throughput
â”œâ”€â”€ Streaming for low time-to-first-token
â”œâ”€â”€ KV cache for context reuse

Scale Challenges:
â”œâ”€â”€ GPU cost dominates (~$3/hour per A100)
â”œâ”€â”€ Memory limits context length
â”œâ”€â”€ Cold starts for model loading
â”œâ”€â”€ Handling traffic spikes
```

---

## ðŸ“– Next Steps

â†’ Return to [Design Problems README](./README.md)
