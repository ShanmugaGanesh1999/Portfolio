# Resource Estimation Examples

> Practical examples of back-of-the-envelope calculations

---

## ðŸ“ Example 1: Twitter-like Service

### Requirements
Design a service like Twitter with:
- 300 million monthly active users (MAU)
- 50% are daily active users (DAU)

### Traffic Estimation

```
DAU = 300M Ã— 50% = 150M users

Actions per day per user:
â”œâ”€â”€ View timeline: 10 times
â”œâ”€â”€ Post tweet: 0.5 tweets (avg)
â”œâ”€â”€ Like/RT: 5 times

Total daily requests:
â”œâ”€â”€ Timeline views: 150M Ã— 10 = 1.5B
â”œâ”€â”€ Post tweets: 150M Ã— 0.5 = 75M
â”œâ”€â”€ Engagement: 150M Ã— 5 = 750M
â””â”€â”€ Total: ~2.3B requests/day

Requests per second:
â”œâ”€â”€ Average: 2.3B Ã· 86,400 = ~27,000 RPS
â”œâ”€â”€ Peak (3x): ~81,000 RPS
```

### Storage Estimation

```
New tweets per day: 75M

Tweet size:
â”œâ”€â”€ Tweet ID: 8 bytes
â”œâ”€â”€ User ID: 8 bytes
â”œâ”€â”€ Text (280 chars): 280 bytes
â”œâ”€â”€ Timestamp: 8 bytes
â”œâ”€â”€ Metadata: ~200 bytes
â””â”€â”€ Total: ~500 bytes

Daily tweet storage:
75M Ã— 500 bytes = 37.5 GB/day

Yearly tweet storage:
37.5 GB Ã— 365 = ~14 TB/year

With 3x replication: ~42 TB/year
```

### Media Storage

```
Assume 10% of tweets have images:
â”œâ”€â”€ 75M Ã— 10% = 7.5M images/day
â”œâ”€â”€ Average image: 200 KB
â””â”€â”€ Daily: 7.5M Ã— 200KB = 1.5 TB/day

With video (1% of tweets):
â”œâ”€â”€ 75M Ã— 1% = 750K videos/day
â”œâ”€â”€ Average video: 5 MB
â””â”€â”€ Daily: 750K Ã— 5MB = 3.75 TB/day

Total media per day: ~5.25 TB
Yearly: ~1.9 PB
```

### Bandwidth Estimation

```
Read-heavy (100:1 read:write ratio):

Outgoing (reads):
â”œâ”€â”€ Timeline: 1.5B Ã— 10 tweets Ã— 500 bytes = 7.5 TB/day
â”œâ”€â”€ Media views: Assume 50% of tweets viewed have media
â”œâ”€â”€ Images: 7.5B Ã— 50% Ã— 200KB = 750 TB/day

Peak bandwidth:
â”œâ”€â”€ Average: 750 TB Ã· 86,400s = 8.7 GB/s
â”œâ”€â”€ Peak (3x): ~26 GB/s = ~210 Gbps
```

---

## ðŸ“ Example 2: URL Shortener (TinyURL)

### Requirements
- 100 million new URLs per month
- 10:1 read:write ratio
- URLs stored for 5 years

### Traffic Estimation

```
Writes:
â”œâ”€â”€ 100M URLs/month
â”œâ”€â”€ 100M Ã· 30 days = 3.3M URLs/day
â”œâ”€â”€ 3.3M Ã· 86,400 = ~40 URLs/second

Reads (10:1 ratio):
â”œâ”€â”€ 400 reads/second average
â”œâ”€â”€ Peak (5x): 2,000 reads/second
```

### Storage Estimation

```
URL record:
â”œâ”€â”€ Short URL (7 chars): 7 bytes
â”œâ”€â”€ Long URL (avg 200 chars): 200 bytes
â”œâ”€â”€ User ID: 8 bytes
â”œâ”€â”€ Timestamp: 8 bytes
â”œâ”€â”€ Click count: 4 bytes
â””â”€â”€ Total: ~230 bytes (round to 250)

Monthly storage:
100M Ã— 250 bytes = 25 GB/month

5-year storage:
25 GB Ã— 60 months = 1.5 TB

With 3x replication: 4.5 TB
```

### URL Key Space

```
How many unique short URLs needed?

5 years Ã— 100M/month = 6 billion URLs

Base62 encoding (a-z, A-Z, 0-9):
â”œâ”€â”€ 6 chars: 62â¶ = 56.8 billion âœ“
â”œâ”€â”€ 7 chars: 62â· = 3.5 trillion âœ“âœ“

7 characters is more than enough
```

### Cache Estimation

```
80/20 rule: 20% of URLs = 80% of traffic

Hot URLs to cache:
â”œâ”€â”€ Total URLs (5 years): 6 billion
â”œâ”€â”€ 20% hot: 1.2 billion URLs
â”œâ”€â”€ Size: 1.2B Ã— 250 bytes = 300 GB

Can fit in memory with a few servers!
```

---

## ðŸ“ Example 3: Instagram/Photo Sharing

### Requirements
- 500 million DAU
- Users upload 2 photos/day on average
- Users view 100 photos/day

### Traffic Estimation

```
Uploads:
â”œâ”€â”€ 500M Ã— 2 = 1 billion photos/day
â”œâ”€â”€ 1B Ã· 86,400 = ~12,000 uploads/second

Views:
â”œâ”€â”€ 500M Ã— 100 = 50 billion views/day
â”œâ”€â”€ 50B Ã· 86,400 = ~580,000 views/second
â”œâ”€â”€ Peak (3x): ~1.7 million views/second
```

### Storage Estimation

```
Photo sizes (store multiple versions):
â”œâ”€â”€ Original: 2 MB
â”œâ”€â”€ High-res: 500 KB
â”œâ”€â”€ Medium: 200 KB
â”œâ”€â”€ Thumbnail: 20 KB
â””â”€â”€ Total per photo: ~2.7 MB

Daily storage:
1B Ã— 2.7 MB = 2.7 PB/day

With 3x replication: 8.1 PB/day

Yearly: ~3,000 PB = 3 EB (Exabytes!)
```

### Bandwidth Estimation

```
Assuming most views are medium size (200KB):

Views bandwidth:
â”œâ”€â”€ 50B Ã— 200 KB = 10 PB/day
â”œâ”€â”€ 10 PB Ã· 86,400 = 116 GB/s
â”œâ”€â”€ Peak: ~350 GB/s = 2.8 Tbps

This is why CDNs are essential!
```

---

## ðŸ“ Example 4: Chat Application (WhatsApp)

### Requirements
- 500 million DAU
- Average 50 messages sent per user per day
- Message size: 100 bytes (text)

### Traffic Estimation

```
Messages per day:
â”œâ”€â”€ 500M Ã— 50 = 25 billion messages/day

Messages per second:
â”œâ”€â”€ 25B Ã· 86,400 = ~290,000 messages/second
â”œâ”€â”€ Peak: ~900,000 messages/second
```

### Storage Estimation

```
Message record:
â”œâ”€â”€ Message ID: 8 bytes
â”œâ”€â”€ Sender ID: 8 bytes
â”œâ”€â”€ Receiver ID: 8 bytes
â”œâ”€â”€ Text: 100 bytes
â”œâ”€â”€ Timestamp: 8 bytes
â”œâ”€â”€ Status: 1 byte
â””â”€â”€ Total: ~135 bytes

Daily storage:
25B Ã— 135 bytes = 3.4 TB/day

Keep 30 days on fast storage:
3.4 TB Ã— 30 = ~100 TB

Archive older messages to cold storage
```

### Connection Estimation

```
Concurrent connections:
â”œâ”€â”€ 500M DAU
â”œâ”€â”€ Peak: 30% online simultaneously
â”œâ”€â”€ 500M Ã— 30% = 150 million connections

Per server (assuming 100K connections each):
â”œâ”€â”€ 150M Ã· 100K = 1,500 servers

Just for WebSocket connections!
```

---

## ðŸ“ Example 5: Video Streaming (YouTube)

### Requirements
- 2 billion MAU, 500M DAU
- Average watch time: 30 minutes/day
- 500K new videos uploaded per day

### Viewing Traffic

```
Total watch time:
â”œâ”€â”€ 500M users Ã— 30 min = 15 billion minutes/day
â”œâ”€â”€ = 250 million hours/day

Assuming average bitrate 5 Mbps:
â”œâ”€â”€ 250M hours Ã— 60 min Ã— 60 sec = 900B seconds
â”œâ”€â”€ 900B Ã— 5 Mbps / 8 = 562 PB/day outbound

Concurrent viewers (peak):
â”œâ”€â”€ Assume peak has 20% DAU watching
â”œâ”€â”€ 500M Ã— 20% = 100 million concurrent
â”œâ”€â”€ 100M Ã— 5 Mbps = 500 Tbps bandwidth
```

### Upload/Storage

```
New videos per day: 500K

Assuming average video is 10 minutes:
â”œâ”€â”€ Upload size: 1 GB (raw)
â”œâ”€â”€ After transcoding: 500 MB (multiple formats)

Daily ingestion:
â”œâ”€â”€ 500K Ã— 1 GB = 500 TB/day uploads

With transcoding (multiple resolutions):
â”œâ”€â”€ 500K Ã— 500 MB = 250 TB/day storage
â”œâ”€â”€ Yearly: ~90 PB
```

---

## ðŸ“ Estimation Template

Use this template for any system:

```markdown
## 1. Requirements
- Users: ___ (DAU/MAU)
- Actions per user: ___
- Data per action: ___
- Retention period: ___

## 2. Traffic
- Daily actions: Users Ã— Actions
- RPS: Daily Ã· 86,400
- Peak RPS: Average Ã— 3 (or 5)
- Read:Write ratio: ___

## 3. Storage
- Per record: ___ bytes
- Daily: Records Ã— Size
- Yearly: Daily Ã— 365
- With replication: Ã— 3

## 4. Bandwidth
- Outgoing: Reads Ã— Size
- Incoming: Writes Ã— Size
- Peak: Average Ã— 3

## 5. Summary Table
| Metric | Value |
|--------|-------|
| Peak RPS | |
| Storage/year | |
| Peak bandwidth | |
| Servers needed | |
```

---

## âš ï¸ Common Pitfalls

### 1. Forgetting Replication

```
Raw storage: 10 TB
With 3x replication: 30 TB
With backups: 60 TB

Always multiply for redundancy!
```

### 2. Ignoring Peak Traffic

```
Average: 10,000 RPS
Peak (viral event): 100,000 RPS

Design for peak, not average
But don't over-provision for rare events
```

### 3. Forgetting Metadata

```
Storing 1M photos:
â”œâ”€â”€ Photo data: 1M Ã— 2 MB = 2 TB
â”œâ”€â”€ Metadata (indexes, thumbnails): +500 GB
â”œâ”€â”€ Total: 2.5 TB
```

### 4. Wrong Units

```
Common confusion:
â”œâ”€â”€ Mbps vs MBps (8x difference)
â”œâ”€â”€ 1000 vs 1024 (binary vs decimal)
â”œâ”€â”€ Per second vs per day

Always clarify units!
```

---

## ðŸ’¡ Interview Tips

### Step-by-Step Approach

1. **State assumptions clearly**
   > "I'll assume 100M DAU with 10 actions per user"

2. **Do math out loud**
   > "100M Ã— 10 = 1B actions per day"

3. **Round appropriately**
   > "1B Ã· 86,400 â‰ˆ 12,000 RPS, let's say 10K for easy math"

4. **Sanity check**
   > "10K RPS is manageable with 10-20 servers"

### Quick Estimation Checks

```
âœ“ Is RPS reasonable? (10-100K for most systems)
âœ“ Is storage reasonable? (TBs to PBs for large systems)
âœ“ Is bandwidth achievable? (Gbps to Tbps)
âœ“ Are server counts practical? (10s to 1000s)
```

---

## âœ… Key Takeaways

1. **Start with users** - Everything derives from user count
2. **Calculate traffic first** - RPS determines architecture
3. **Storage adds up** - Small records Ã— millions = lots of data
4. **Peak matters** - Design for 3-5x average load
5. **Replication multiplies** - Always factor in redundancy
6. **Round generously** - These are estimates, not exact
