# Blob Store

> Storing and retrieving large binary objects at scale

---

## ğŸ“– What is a Blob Store?

A **Blob Store** (Binary Large Object Store) is a storage system optimized for storing unstructured data like images, videos, documents, backups, and logs.

```
Blob Store Characteristics:
â”œâ”€â”€ Flat namespace (no hierarchy in storage)
â”œâ”€â”€ Optimized for large files
â”œâ”€â”€ Immutable objects (replace, not modify)
â”œâ”€â”€ Highly durable (99.999999999% - 11 nines)
â””â”€â”€ Globally accessible via HTTP
```

---

## ğŸ¯ Blob vs File vs Block Storage

```
Block Storage (EBS, SAN):
â”œâ”€â”€ Fixed-size blocks
â”œâ”€â”€ Attached to single server
â”œâ”€â”€ Low latency
â””â”€â”€ Use: Databases, OS disks

File Storage (NFS, EFS):
â”œâ”€â”€ Hierarchical (folders)
â”œâ”€â”€ Shared access
â”œâ”€â”€ POSIX compatible
â””â”€â”€ Use: Shared files, home directories

Blob Storage (S3, GCS):
â”œâ”€â”€ Flat namespace (buckets + keys)
â”œâ”€â”€ HTTP access
â”œâ”€â”€ Massive scale
â””â”€â”€ Use: Images, videos, backups
```

```
                   Block            File             Blob
Abstraction:       Blocks           Files            Objects
Access:            Mount            Mount/Network    HTTP API
Scale:             Limited          Medium           Unlimited
Use Case:          Database         Shared Files     Media/Backups
```

---

## ğŸ”§ Blob Store Architecture

### Basic Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Blob Store                            â”‚
â”‚                                                              â”‚
â”‚   Bucket: my-images                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚  Object Key           â”‚  Data        â”‚  Metadata      â”‚ â”‚
â”‚   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚
â”‚   â”‚  photos/cat.jpg       â”‚  [binary]    â”‚  size, type    â”‚ â”‚
â”‚   â”‚  photos/dog.jpg       â”‚  [binary]    â”‚  size, type    â”‚ â”‚
â”‚   â”‚  avatars/user-123.png â”‚  [binary]    â”‚  size, type    â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â”‚   Bucket: my-videos                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚  videos/intro.mp4     â”‚  [binary]    â”‚  size, type    â”‚ â”‚
â”‚   â”‚  videos/demo.mp4      â”‚  [binary]    â”‚  size, type    â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Access: https://my-images.s3.amazonaws.com/photos/cat.jpg
```

### Internal Architecture

```
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    Client â”€â”€â”€REST APIâ”€â”€â”€â–ºâ”‚   Front-End     â”‚
                          â”‚   (API Layer)   â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼                  â–¼                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Metadata â”‚       â”‚  Index   â”‚       â”‚   Auth   â”‚
         â”‚  Service â”‚       â”‚ Service  â”‚       â”‚ Service  â”‚
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                  â”‚
              â–¼                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      Metadata Database      â”‚
         â”‚   (Object â†’ Location map)   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚       Storage Layer          â”‚
         â”‚                              â”‚
         â”‚   Chunk 1    Chunk 2         â”‚
         â”‚  [Server A] [Server B]       â”‚
         â”‚  (replica)  (replica)        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Key Operations

### Upload (PUT)

```
1. Client sends: PUT /bucket/key with data
2. Front-end validates request
3. Data chunked into parts
4. Each chunk replicated to multiple nodes
5. Metadata stored (key â†’ chunk locations)
6. Return success + ETag

Large File Upload (Multipart):
â”œâ”€â”€ Initiate upload â†’ get upload ID
â”œâ”€â”€ Upload parts in parallel
â”œâ”€â”€ Complete upload â†’ combine parts
â””â”€â”€ Abort if failed
```

```python
# AWS S3 multipart upload example
import boto3
from boto3.s3.transfer import TransferConfig

s3 = boto3.client('s3')

# Config for multipart (files > 8MB split into parts)
config = TransferConfig(
    multipart_threshold=8 * 1024 * 1024,  # 8MB
    multipart_chunksize=8 * 1024 * 1024,
    max_concurrency=10
)

s3.upload_file(
    'large_video.mp4',
    'my-bucket',
    'videos/large_video.mp4',
    Config=config
)
```

### Download (GET)

```
1. Client sends: GET /bucket/key
2. Lookup metadata â†’ find chunk locations
3. Fetch chunks from storage nodes
4. Assemble and return data

Range Requests (partial download):
GET /bucket/key
Range: bytes=0-999999

Returns first 1MB only
Used for: Video streaming, resumable downloads
```

### Delete

```
Soft Delete:
â”œâ”€â”€ Mark object as deleted
â”œâ”€â”€ Actually delete after retention period
â””â”€â”€ Allows recovery

Hard Delete:
â”œâ”€â”€ Remove immediately
â”œâ”€â”€ May keep versions (if versioning enabled)
```

---

## ğŸ”§ Data Organization

### Chunking

```
Large file split into chunks:

Original: 100MB file
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
   â”‚16MB â”‚16MB â”‚16MB â”‚16MB â”‚16MB â”‚... (chunks)
   â””â”€â”€â”¬â”€â”€â”´â”€â”€â”¬â”€â”€â”´â”€â”€â”¬â”€â”€â”´â”€â”€â”¬â”€â”€â”´â”€â”€â”¬â”€â”€â”˜
      â”‚     â”‚     â”‚     â”‚     â”‚
      â–¼     â–¼     â–¼     â–¼     â–¼
   Node A Node B Node C Node A Node B

Benefits:
â”œâ”€â”€ Parallel upload/download
â”œâ”€â”€ Efficient storage (dedup)
â”œâ”€â”€ Fault tolerance (lose chunk, not file)
â””â”€â”€ Resume from any point
```

### Replication

```
Each chunk replicated N times:

Chunk 1:
â”œâ”€â”€ Replica 1 â†’ Datacenter A, Rack 1, Node 3
â”œâ”€â”€ Replica 2 â†’ Datacenter A, Rack 4, Node 7
â””â”€â”€ Replica 3 â†’ Datacenter B, Rack 2, Node 1

Placement rules:
â”œâ”€â”€ Different racks (survive rack failure)
â”œâ”€â”€ Different datacenters (survive DC failure)
â””â”€â”€ Different regions (survive region failure)
```

### Erasure Coding

```
Alternative to replication for cold data:

Replication (3 copies): 100MB Ã— 3 = 300MB storage

Erasure Coding (4+2):
â”œâ”€â”€ Split into 4 data chunks
â”œâ”€â”€ Create 2 parity chunks
â”œâ”€â”€ Total: 6 chunks
â”œâ”€â”€ Can lose any 2, still recover
â””â”€â”€ Storage: 100MB Ã— 1.5 = 150MB

Trade-off:
â”œâ”€â”€ More CPU for encode/decode
â”œâ”€â”€ Less storage cost
â””â”€â”€ Best for rarely accessed data
```

---

## ğŸ“ˆ Blob Store Services

### Amazon S3

```
Features:
â”œâ”€â”€ 11 nines durability (99.999999999%)
â”œâ”€â”€ Storage classes (Standard, IA, Glacier)
â”œâ”€â”€ Versioning
â”œâ”€â”€ Lifecycle policies
â”œâ”€â”€ Cross-region replication
â””â”€â”€ Event notifications

Storage Classes:
â”œâ”€â”€ Standard: Frequent access (~$0.023/GB)
â”œâ”€â”€ Infrequent Access: ~$0.0125/GB + retrieval fee
â”œâ”€â”€ Glacier: Archive, ~$0.004/GB, minutes-hours retrieval
â””â”€â”€ Glacier Deep Archive: ~$0.00099/GB, 12hr retrieval
```

### Google Cloud Storage

```
Similar to S3:
â”œâ”€â”€ Standard, Nearline, Coldline, Archive classes
â”œâ”€â”€ Object lifecycle management
â”œâ”€â”€ Strong consistency
â””â”€â”€ Signed URLs
```

### Azure Blob Storage

```
Tiers:
â”œâ”€â”€ Hot: Frequent access
â”œâ”€â”€ Cool: Infrequent access
â”œâ”€â”€ Archive: Rarely accessed
â””â”€â”€ Premium: High performance
```

---

## ğŸ”§ Access Control

### Bucket Policies

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": "*",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::my-bucket/public/*"
    }
  ]
}
```

### Signed URLs

```
Grant temporary access to private objects:

URL = base + object + signature + expiry

https://my-bucket.s3.amazonaws.com/private/doc.pdf
  ?X-Amz-Signature=abc123...
  &X-Amz-Expires=3600

Anyone with URL can access for 1 hour
No AWS credentials needed
```

```python
# Generate signed URL
import boto3

s3 = boto3.client('s3')
url = s3.generate_presigned_url(
    'get_object',
    Params={'Bucket': 'my-bucket', 'Key': 'private/doc.pdf'},
    ExpiresIn=3600  # 1 hour
)
```

---

## ğŸ’¡ Common Patterns

### 1. Direct Upload

```
Problem: Uploading through your server is slow

Solution: Client uploads directly to blob store

1. Client requests upload URL from your server
2. Server generates signed PUT URL
3. Client uploads directly to S3
4. S3 notifies your server (optional)

    Client â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º S3
            (direct upload)
       â”‚                               â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼   â–¼ (notification)
                Your Server
```

### 2. CDN Integration

```
    User â”€â”€â–º CDN Edge â”€â”€â–º Origin (S3)
               â”‚
               â–¼
           (cached)
           
Subsequent requests served from edge
Lower latency, reduced S3 costs
```

### 3. Image Processing Pipeline

```
Upload â”€â”€â–º S3 â”€â”€â–º Lambda (triggered) â”€â”€â–º Generate Thumbnails
                                               â”‚
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â–¼                       â–¼
                          s3://thumbs/small/        s3://thumbs/large/
```

### 4. Data Lake

```
Store everything in blob store:

s3://data-lake/
â”œâ”€â”€ raw/                    # Original data
â”‚   â”œâ”€â”€ logs/2024/01/01/
â”‚   â””â”€â”€ events/2024/01/01/
â”œâ”€â”€ processed/              # Cleaned data
â”‚   â””â”€â”€ parquet/
â””â”€â”€ curated/               # Analytics-ready
    â””â”€â”€ tables/

Query with: Athena, Presto, Spark
```

---

## âš ï¸ Considerations

### Naming

```
Object keys are just strings:

Good:
â”œâ”€â”€ photos/{user_id}/{timestamp}_{filename}
â”œâ”€â”€ logs/{year}/{month}/{day}/access.log
â””â”€â”€ Use prefixes for organization

Bad:
â”œâ”€â”€ Sequential keys (hot partitions)
â”œâ”€â”€ Special characters that need encoding
â””â”€â”€ Very long keys
```

### Consistency

```
S3 is now strongly consistent (since 2020):
â”œâ”€â”€ Read-after-write consistent
â”œâ”€â”€ List operations consistent
â””â”€â”€ No more eventual consistency issues

Other systems may vary - check docs!
```

### Cost Optimization

```
1. Choose right storage class
2. Lifecycle policies (auto-move to cheaper class)
3. Enable intelligent tiering
4. Delete incomplete multipart uploads
5. Use transfer acceleration for speed
6. Compress before storing
```

---

## ğŸ’¡ In System Design Interviews

### When to Use Blob Store

```
1. "Store images/videos/files"
2. "Need cheap, durable storage"
3. "Files accessed via HTTP"
4. "Scale to petabytes"
```

### Design Points

```
1. What to store in blob store vs database?
   â†’ Large files (images, videos) â†’ Blob
   â†’ Metadata â†’ Database

2. Access patterns?
   â†’ Read-heavy â†’ Add CDN
   â†’ Write-heavy â†’ Use multipart upload

3. Security?
   â†’ Private buckets + signed URLs

4. Cost?
   â†’ Lifecycle policies, storage classes
```

---

## âœ… Key Takeaways

1. **Blob store = unstructured data** at massive scale
2. **Flat namespace** - buckets and keys, no real folders
3. **Immutable objects** - replace, don't modify
4. **11 nines durability** - your data is safe
5. **Use CDN** for frequently accessed content
6. **Direct upload** with signed URLs saves bandwidth
7. **Storage classes** for cost optimization
8. **Metadata in database**, files in blob store
