# Distributed Logging

> Collecting, storing, and analyzing logs at scale

---

## ðŸ“– What is Distributed Logging?

**Distributed Logging** is a system that collects, aggregates, stores, and analyzes logs from multiple services across a distributed system.

```
Without Centralized Logging:
â”œâ”€â”€ SSH into each server
â”œâ”€â”€ grep through local files
â”œâ”€â”€ Manually correlate across services
â””â”€â”€ Nightmare to debug!

With Distributed Logging:
â”œâ”€â”€ Single dashboard for all logs
â”œâ”€â”€ Search across all services
â”œâ”€â”€ Correlate with request IDs
â””â”€â”€ Alerts on patterns
```

---

## ðŸŽ¯ Why Distributed Logging?

```
Challenges in Distributed Systems:
â”œâ”€â”€ Many services generate logs
â”œâ”€â”€ Containers are ephemeral (logs disappear)
â”œâ”€â”€ Need to trace requests across services
â”œâ”€â”€ Debug issues across multiple components
â””â”€â”€ Compliance and audit requirements

Benefits:
â”œâ”€â”€ Single pane of glass
â”œâ”€â”€ Fast search and filtering
â”œâ”€â”€ Pattern detection and alerting
â”œâ”€â”€ Historical analysis
â””â”€â”€ Debugging production issues
```

---

## ðŸ”§ Logging Architecture

### Basic Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Logging Pipeline                          â”‚
â”‚                                                              â”‚
â”‚   Generate        Collect        Process       Store         â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚Service Aâ”‚â”€â”€â”€â–ºâ”‚         â”‚    â”‚         â”‚   â”‚         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚         â”‚    â”‚         â”‚   â”‚         â”‚   â”‚
â”‚                 â”‚ Agent/  â”‚â”€â”€â”€â–ºâ”‚ Process â”‚â”€â”€â–ºâ”‚ Storage â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚ Shipper â”‚    â”‚ (Parse) â”‚   â”‚ (Index) â”‚   â”‚
â”‚  â”‚Service Bâ”‚â”€â”€â”€â–ºâ”‚         â”‚    â”‚         â”‚   â”‚         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚         â”‚    â”‚         â”‚   â”‚         â”‚   â”‚
â”‚                 â”‚ Fluentd â”‚    â”‚ Logstashâ”‚   â”‚  Elasticâ”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚ Filebeatâ”‚    â”‚         â”‚   â”‚         â”‚   â”‚
â”‚  â”‚Service Câ”‚â”€â”€â”€â–ºâ”‚ etc.    â”‚    â”‚         â”‚   â”‚         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                     â”‚        â”‚
â”‚                                              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                                              â”‚   Kibana    â”‚ â”‚
â”‚                                              â”‚ (Visualize) â”‚ â”‚
â”‚                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ELK Stack (Most Common)

```
E - Elasticsearch: Storage & Search
L - Logstash: Processing & Transformation
K - Kibana: Visualization & Dashboards

Also: EFK (Fluentd instead of Logstash)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Servicesâ”‚â”€â”€â”€â–ºâ”‚ Filebeat â”‚â”€â”€â”€â–ºâ”‚   Logstash    â”‚â”€â”€â”€â–ºâ”‚Elastic â”‚
â”‚  (logs) â”‚    â”‚ (ship)   â”‚    â”‚(parse/filter) â”‚    â”‚search  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
                                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
                                                   â”‚ Kibana â”‚
                                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“Š Log Collection

### Collection Methods

```
1. File-based:
   Service writes to file â†’ Agent tails file
   â”œâ”€â”€ Filebeat, Fluentd, Logstash
   â””â”€â”€ Most common for traditional apps

2. Sidecar Container (Kubernetes):
   App container â†’ Shared volume â†’ Sidecar â†’ Ship
   â””â”€â”€ Good for containerized apps

3. Direct Shipping:
   App â†’ SDK â†’ Logging Service
   â””â”€â”€ More control, higher coupling

4. Stdout/Stderr (Containers):
   Container logs â†’ Docker/K8s â†’ Ship
   â””â”€â”€ Native container logging
```

### Agent Configuration (Filebeat Example)

```yaml
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/log/app/*.log
    fields:
      service: user-service
      environment: production
    
    # Multiline logs (stack traces)
    multiline.pattern: '^\d{4}-\d{2}-\d{2}'
    multiline.negate: true
    multiline.match: after

output.logstash:
  hosts: ["logstash:5044"]
```

---

## ðŸ”§ Log Processing

### Parsing Unstructured Logs

```
Raw log:
2024-01-15 10:30:45 INFO [user-service] User 123 logged in from 192.168.1.1

Parsed (structured):
{
  "timestamp": "2024-01-15T10:30:45Z",
  "level": "INFO",
  "service": "user-service",
  "message": "User 123 logged in from 192.168.1.1",
  "user_id": "123",
  "ip": "192.168.1.1"
}
```

### Logstash Pipeline

```ruby
# Input
input {
  beats {
    port => 5044
  }
}

# Filter (parse/transform)
filter {
  # Parse timestamp and fields
  grok {
    match => { 
      "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} \[%{DATA:service}\] %{GREEDYDATA:msg}" 
    }
  }
  
  # Parse date
  date {
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
  }
  
  # Add geo info from IP
  geoip {
    source => "client_ip"
  }
  
  # Remove sensitive data
  mutate {
    remove_field => ["password", "credit_card"]
  }
}

# Output
output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
}
```

---

## ðŸ“ˆ Structured Logging

### Why Structured Logs?

```
Unstructured (bad for parsing):
"User john@email.com placed order #12345 for $99.99"

Structured (JSON - easy to parse):
{
  "event": "order_placed",
  "user_email": "john@email.com",
  "order_id": "12345",
  "amount": 99.99,
  "currency": "USD",
  "timestamp": "2024-01-15T10:30:45Z"
}

Benefits:
â”œâ”€â”€ No parsing needed
â”œâ”€â”€ Easy to search/filter
â”œâ”€â”€ Consistent schema
â””â”€â”€ Better analytics
```

### Application Logging Best Practices

```python
import structlog
import uuid

# Configure structured logging
logger = structlog.get_logger()

# Add correlation ID for request tracing
def process_order(order_id, user_id):
    correlation_id = str(uuid.uuid4())
    
    log = logger.bind(
        correlation_id=correlation_id,
        order_id=order_id,
        user_id=user_id
    )
    
    log.info("processing_order_started")
    
    try:
        # Process...
        log.info("payment_processed", amount=99.99)
        log.info("order_completed", status="success")
    except Exception as e:
        log.error("order_failed", error=str(e))
        raise
```

---

## ðŸ”§ Log Levels

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Level  â”‚ When to Use                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TRACE  â”‚ Very detailed debugging (not for production)    â”‚
â”‚ DEBUG  â”‚ Detailed flow, useful for debugging             â”‚
â”‚ INFO   â”‚ Normal operations, milestones                   â”‚
â”‚ WARN   â”‚ Potential issues, degraded but working          â”‚
â”‚ ERROR  â”‚ Failures that need attention                    â”‚
â”‚ FATAL  â”‚ System crash, immediate attention needed        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Production typically: INFO and above
Debugging: DEBUG and above
Never in prod: TRACE (too verbose)
```

---

## ðŸ“Š Log Storage

### Index Strategy

```
Time-based indices:
logs-2024.01.15
logs-2024.01.16
logs-2024.01.17

Benefits:
â”œâ”€â”€ Easy retention management (delete old indices)
â”œâ”€â”€ Better query performance (search specific days)
â””â”€â”€ Can have different policies per age

Index Lifecycle Management (ILM):
Hot    (0-7 days):   Fast storage, all replicas
Warm   (7-30 days):  Fewer replicas, slower storage
Cold   (30-90 days): Minimal resources, compressed
Delete (90+ days):   Remove
```

### Retention Policies

```
Consider:
â”œâ”€â”€ Compliance requirements (GDPR, SOC2, HIPAA)
â”œâ”€â”€ Storage costs
â”œâ”€â”€ Query patterns (how far back do you search?)
â””â”€â”€ Legal hold requirements

Typical:
â”œâ”€â”€ Application logs: 30-90 days
â”œâ”€â”€ Security logs: 1 year
â”œâ”€â”€ Audit logs: 7 years
â””â”€â”€ Debug logs: 7 days
```

---

## ðŸ’¡ Distributed Tracing Integration

### Correlation IDs

```
Track a request across services:

Request â†’ API Gateway â†’ User Service â†’ Order Service â†’ DB
     â”‚         â”‚              â”‚              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              correlation_id: abc-123

All logs include same correlation_id
Easy to search: correlation_id:abc-123
See entire request flow
```

```python
# Middleware to propagate correlation ID
def correlation_middleware(request, next):
    correlation_id = (
        request.headers.get('X-Correlation-ID') or 
        str(uuid.uuid4())
    )
    
    # Store in thread local for this request
    context.correlation_id = correlation_id
    
    # Pass to downstream services
    response = next(request)
    response.headers['X-Correlation-ID'] = correlation_id
    
    return response
```

### With Distributed Tracing

```
Logs + Traces together:

Trace ID: abc-123
â”œâ”€â”€ Span: API Gateway (50ms)
â”‚   â””â”€â”€ Logs: "Request received", "Auth passed"
â”œâ”€â”€ Span: User Service (30ms)
â”‚   â””â”€â”€ Logs: "User fetched", "Permissions checked"
â””â”€â”€ Span: Order Service (100ms)
    â””â”€â”€ Logs: "Order created", "Payment processed"

Tools: Jaeger, Zipkin, AWS X-Ray
Link logs to traces for full context
```

---

## ðŸ”§ Alerting

### Log-based Alerts

```
Alert on patterns:

1. Error rate spike:
   "More than 100 ERROR logs in 5 minutes"

2. Specific error:
   "OutOfMemoryError detected"

3. Security events:
   "Multiple failed login attempts from same IP"

4. Business events:
   "Payment failure rate > 5%"
```

### Alert Configuration (ElastAlert Example)

```yaml
name: High Error Rate
type: frequency
index: logs-*
num_events: 100
timeframe:
  minutes: 5

filter:
  - term:
      level: ERROR

alert:
  - slack:
      slack_webhook_url: "https://hooks.slack.com/..."
      
alert_text: |
  High error rate detected!
  Errors in last 5 min: {0}
  
realert:
  minutes: 30  # Don't spam
```

---

## ðŸ“ˆ Log Management Services

### Self-Hosted

```
ELK/EFK Stack:
â”œâ”€â”€ Full control
â”œâ”€â”€ No data leaves your infra
â”œâ”€â”€ Operational overhead
â””â”€â”€ Scaling complexity

Loki (Grafana):
â”œâ”€â”€ Log aggregation for Prometheus users
â”œâ”€â”€ Labels-based (like Prometheus)
â”œâ”€â”€ Cost-effective (no full-text index)
â””â”€â”€ Good for Kubernetes
```

### Managed Services

```
Datadog:
â”œâ”€â”€ Logs + Metrics + Traces
â”œâ”€â”€ Easy setup
â”œâ”€â”€ Expensive at scale
â””â”€â”€ Great UI/UX

Splunk:
â”œâ”€â”€ Enterprise standard
â”œâ”€â”€ Powerful SPL query language
â”œâ”€â”€ Very expensive
â””â”€â”€ Great for security

AWS CloudWatch:
â”œâ”€â”€ Native AWS integration
â”œâ”€â”€ Pay per ingestion/storage
â”œâ”€â”€ Limited query capabilities
â””â”€â”€ Good for AWS-native

Others: Sumo Logic, Papertrail, Loggly
```

### Comparison

| Service | Best For | Cost | Query Power |
|---------|----------|------|-------------|
| ELK | Full control | Medium | High |
| Loki | K8s + Prometheus | Low | Medium |
| Datadog | All-in-one | High | High |
| Splunk | Enterprise/Security | Very High | Very High |
| CloudWatch | AWS native | Medium | Low |

---

## ðŸ’¡ Best Practices

### What to Log

```
DO log:
â”œâ”€â”€ Request/response metadata (not bodies)
â”œâ”€â”€ Errors with context
â”œâ”€â”€ Business events
â”œâ”€â”€ Performance metrics
â”œâ”€â”€ Security events
â””â”€â”€ State transitions

DON'T log:
â”œâ”€â”€ Sensitive data (passwords, tokens)
â”œâ”€â”€ PII without redaction
â”œâ”€â”€ High-frequency debug in prod
â”œâ”€â”€ Large payloads (use sampling)
â””â”€â”€ Secrets, API keys
```

### Performance Considerations

```
1. Async logging (don't block app)
2. Batch writes to reduce I/O
3. Sample high-volume logs
4. Compress before shipping
5. Use structured logging (less parsing)
6. Index only what you search
```

---

## ðŸ’¡ In System Design Interviews

### When to Discuss

```
1. "How do you debug issues in production?"
2. "How do you trace requests across services?"
3. "How do you monitor for errors?"
4. "Audit trail requirements?"
```

### Key Points

```
1. Centralized logging is essential for microservices
2. Use structured logging (JSON)
3. Correlation IDs for request tracing
4. Log levels appropriate for environment
5. Retention based on requirements
6. Alert on patterns, not individual logs
7. Don't log sensitive data
```

---

## âœ… Key Takeaways

1. **Centralized logging** is essential for distributed systems
2. **ELK/EFK stack** is the most common solution
3. **Structured logging** (JSON) eliminates parsing issues
4. **Correlation IDs** link logs across services
5. **Time-based indices** for easy retention
6. **Log levels** - INFO+ in production
7. **Never log** passwords, tokens, PII
8. **Alert on patterns** not individual logs

---

## ðŸ“– Next Steps

â†’ Continue to [Distributed Monitoring](./14-distributed-monitoring.md)
