# Rate Limiter

> Controlling the rate of requests to protect systems

---

## ðŸ“– What is a Rate Limiter?

A **Rate Limiter** controls how many requests a client can make in a given time window. It protects services from:
- DoS attacks
- Resource exhaustion
- Noisy neighbors
- Cascading failures

```
Without Rate Limiting:
Client â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Server (overwhelmed)
       [10,000 requests/sec]

With Rate Limiting:
Client â”€â”€â”€â”€â”€â–º Rate Limiter â”€â”€â”€â”€â”€â–º Server (protected)
              [allows 100/sec]
              [rejects rest]
```

---

## ðŸŽ¯ Rate Limiting Dimensions

```
Who to limit:
â”œâ”€â”€ User ID (authenticated users)
â”œâ”€â”€ IP Address (anonymous users)
â”œâ”€â”€ API Key (developer apps)
â”œâ”€â”€ Region/Country
â””â”€â”€ Combination of above

What to limit:
â”œâ”€â”€ Requests per second/minute/hour
â”œâ”€â”€ Data transfer (bytes)
â”œâ”€â”€ Concurrent connections
â”œâ”€â”€ Specific expensive operations
â””â”€â”€ Cost-based (API credits)
```

---

## ðŸ”§ Rate Limiting Algorithms

### 1. Token Bucket

```
Most flexible and widely used

Bucket:
â”œâ”€â”€ Holds tokens (capacity = max burst)
â”œâ”€â”€ Tokens added at fixed rate
â”œâ”€â”€ Request consumes 1 token
â”œâ”€â”€ If no tokens: reject

        [â—â—â—â—â—‹â—‹â—‹â—‹â—‹â—‹] Bucket (10 capacity)
             â”‚
             â–¼
        Add 5 tokens/sec
        
Request arrives:
â”œâ”€â”€ Token available â†’ Allow, remove token
â””â”€â”€ No token â†’ Reject (429 Too Many Requests)

Allows bursts up to bucket capacity
```

```python
import time

class TokenBucket:
    def __init__(self, capacity, refill_rate):
        self.capacity = capacity      # Max tokens
        self.tokens = capacity         # Current tokens
        self.refill_rate = refill_rate # Tokens/sec
        self.last_refill = time.time()
    
    def allow_request(self):
        self._refill()
        if self.tokens >= 1:
            self.tokens -= 1
            return True
        return False
    
    def _refill(self):
        now = time.time()
        elapsed = now - self.last_refill
        new_tokens = elapsed * self.refill_rate
        self.tokens = min(self.capacity, self.tokens + new_tokens)
        self.last_refill = now
```

### 2. Leaky Bucket

```
Smooths out bursts, constant output rate

Bucket = Queue:
â”œâ”€â”€ Requests enter bucket (queue)
â”œâ”€â”€ Processed at fixed rate
â”œâ”€â”€ If bucket full: reject

Requests:  â–¼ â–¼ â–¼ â–¼ â–¼ (bursty input)
           â”‚ â”‚ â”‚ â”‚ â”‚
         â”Œâ”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”
         â”‚  [queue]  â”‚ â† Bucket
         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼ â–¼ â–¼ (constant output)
            Process at fixed rate

Difference from Token Bucket:
â”œâ”€â”€ Token: allows bursts
â””â”€â”€ Leaky: no bursts, constant rate
```

### 3. Fixed Window Counter

```
Simple but has edge problem

Window: 1 minute
Limit: 100 requests

Time: 00:00 - 00:59 â†’ Count requests
Time: 01:00 - 01:59 â†’ Reset, new count

Problem - boundary burst:
â”œâ”€â”€ 00:59 â†’ 100 requests (allowed)
â”œâ”€â”€ 01:00 â†’ 100 requests (allowed)
â””â”€â”€ 200 requests in 2 seconds! âŒ

[â”€â”€â”€â”€â”€â”€â”€â”€Window 1â”€â”€â”€â”€â”€â”€â”€â”€][â”€â”€â”€â”€â”€â”€â”€â”€Window 2â”€â”€â”€â”€â”€â”€â”€â”€]
                   100   â”‚100
                    ^â”€â”€â”€â”€â”´â”€â”€â”€â”€^
                    Edge burst problem
```

```python
import time

class FixedWindowCounter:
    def __init__(self, limit, window_seconds):
        self.limit = limit
        self.window = window_seconds
        self.count = 0
        self.window_start = time.time()
    
    def allow_request(self):
        now = time.time()
        # Check if new window
        if now - self.window_start >= self.window:
            self.count = 0
            self.window_start = now
        
        if self.count < self.limit:
            self.count += 1
            return True
        return False
```

### 4. Sliding Window Log

```
Accurate but memory intensive

Keep log of all request timestamps:
[1:00:01, 1:00:02, 1:00:05, 1:00:30, ...]

For each request:
â”œâ”€â”€ Remove timestamps older than window
â”œâ”€â”€ Count remaining timestamps
â”œâ”€â”€ If count < limit â†’ Allow, add timestamp
â””â”€â”€ Else â†’ Reject

Pros: Accurate, no edge problem
Cons: Stores all timestamps (memory)
```

### 5. Sliding Window Counter

```
Best balance: accurate + low memory

Combine fixed window with sliding calculation:

Window 1 (prev): 70 requests
Window 2 (curr): 30 requests so far

Current request at 40% into Window 2:
Weighted count = 30 + (70 Ã— 60%) = 30 + 42 = 72

If limit = 100 â†’ 72 < 100 â†’ Allow

[â”€â”€â”€â”€â”€â”€â”€â”€Window 1â”€â”€â”€â”€â”€â”€â”€â”€][â”€â”€â”€â”€â”€â”€â”€â”€Window 2â”€â”€â”€â”€â”€â”€â”€â”€]
        70 requests       â”‚    30 requests
                          â”‚ â—„â”€â”€40% into window
                          
Estimated count in sliding window:
= current + (previous Ã— overlap%)
```

### Algorithm Comparison

| Algorithm | Memory | Accuracy | Burst Handling |
|-----------|--------|----------|----------------|
| Token Bucket | Low | Good | Allows controlled bursts |
| Leaky Bucket | Low | Good | Smooths all bursts |
| Fixed Window | Very Low | Poor at edges | Allows boundary bursts |
| Sliding Log | High | Exact | No bursts |
| Sliding Counter | Low | Good approximation | Some burst control |

---

## ðŸ”§ Where to Implement

```
1. Client-side:
   â”œâ”€â”€ Prevents wasted requests
   â””â”€â”€ Can be bypassed (not secure)

2. API Gateway:
   â”œâ”€â”€ Centralized
   â”œâ”€â”€ Before hitting services
   â””â”€â”€ Most common location

3. Load Balancer:
   â”œâ”€â”€ Network level
   â””â”€â”€ IP-based limiting

4. Application:
   â”œâ”€â”€ Fine-grained control
   â”œâ”€â”€ Can access user context
   â””â”€â”€ Each server has own view

5. Middleware:
   â”œâ”€â”€ Before request processing
   â””â”€â”€ Easy to add/remove
```

```
Typical architecture:

Client â”€â”€â–º CDN â”€â”€â–º API Gateway â”€â”€â–º Service
            â”‚           â”‚
      IP limit    User/API key limit
```

---

## ðŸ“Š Distributed Rate Limiting

### Challenge

```
Multiple servers, shared limit:

User: 100 req/min limit

Server A: counts 50
Server B: counts 50
                      = User made 100 requests âœ“

But if not synchronized:
Server A: allows 100
Server B: allows 100
                      = User made 200 requests âŒ
```

### Solution: Centralized Counter

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”Œâ”€â”€â”€â”€â”‚    Redis    â”‚â”€â”€â”€â”€â”
           â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
           â–¼                       â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚Server A â”‚             â”‚Server B â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Each request:
1. INCR user:{id}:count
2. Check if over limit
3. Redis handles atomicity
```

```python
# Distributed rate limiter with Redis
import redis
import time

class DistributedRateLimiter:
    def __init__(self, redis_client, limit, window_seconds):
        self.redis = redis_client
        self.limit = limit
        self.window = window_seconds
    
    def allow_request(self, user_id):
        key = f"ratelimit:{user_id}"
        
        # Atomic increment and expire
        pipe = self.redis.pipeline()
        pipe.incr(key)
        pipe.expire(key, self.window)
        results = pipe.execute()
        
        count = results[0]
        return count <= self.limit
```

### Sliding Window with Redis

```lua
-- Lua script for atomic sliding window
local key = KEYS[1]
local now = tonumber(ARGV[1])
local window = tonumber(ARGV[2])
local limit = tonumber(ARGV[3])

-- Remove old entries
redis.call('ZREMRANGEBYSCORE', key, 0, now - window)

-- Count current entries
local count = redis.call('ZCARD', key)

if count < limit then
    -- Add new entry
    redis.call('ZADD', key, now, now)
    redis.call('EXPIRE', key, window)
    return 1  -- Allowed
else
    return 0  -- Rejected
end
```

---

## ðŸ“ˆ Rate Limiting Response

### HTTP Headers

```
Standard headers to include:

X-RateLimit-Limit: 100        # Max requests
X-RateLimit-Remaining: 23     # Requests left
X-RateLimit-Reset: 1672531200 # When limit resets (epoch)
Retry-After: 60               # Seconds to wait (if limited)

Response when limited:
HTTP/1.1 429 Too Many Requests
Retry-After: 60
{
  "error": "Rate limit exceeded",
  "retry_after": 60
}
```

### Graceful Degradation

```
Instead of hard reject:

1. Throttle: Slow down response
2. Queue: Add to queue, process later
3. Deprioritize: Serve after premium users
4. Degrade: Return cached/simplified response
```

---

## ðŸ’¡ Rate Limiting Strategies

### 1. User-based

```
Each user has own limit:
â”œâ”€â”€ Free tier: 100 req/hour
â”œâ”€â”€ Pro tier: 1000 req/hour
â”œâ”€â”€ Enterprise: 10,000 req/hour

Key: user:{user_id}
```

### 2. API Key based

```
Each application has limit:
â”œâ”€â”€ App A: 5000 req/day
â”œâ”€â”€ App B: 10000 req/day

Key: api:{api_key}
```

### 3. Endpoint-based

```
Different limits per endpoint:
â”œâ”€â”€ GET /users: 1000/min (read, cheap)
â”œâ”€â”€ POST /upload: 10/min (write, expensive)
â”œâ”€â”€ POST /analyze: 5/min (compute-heavy)
```

### 4. Cost-based

```
Different requests cost different amounts:

Budget: 1000 credits/minute

Request costs:
â”œâ”€â”€ GET: 1 credit
â”œâ”€â”€ POST: 5 credits
â”œâ”€â”€ Complex query: 50 credits

User uses 20 GETs (20) + 10 POSTs (50) = 70 credits
970 credits remaining
```

---

## ðŸ’¡ In System Design Interviews

### When to Use

```
1. "How do you prevent abuse?"
2. "What if a user sends too many requests?"
3. "How do you protect against DoS?"
4. "How do you ensure fair usage?"
```

### Design Discussion Points

```
1. Where to rate limit?
   â†’ API Gateway for global, app for granular

2. Which algorithm?
   â†’ Token bucket (most versatile)
   â†’ Sliding window counter (good accuracy)

3. How to handle distributed?
   â†’ Redis for centralized counting

4. What to return?
   â†’ 429 with Retry-After header

5. Rate limit by what?
   â†’ User ID, API key, IP (fallback)

6. Different limits?
   â†’ Per endpoint, per tier
```

---

## âš ï¸ Edge Cases

### Race Conditions

```
Problem: Check-then-increment not atomic

Thread A: count = 99, 99 < 100 â†’ Allow
Thread B: count = 99, 99 < 100 â†’ Allow
Both increment â†’ count = 101 âŒ

Solution: Use atomic operations
â”œâ”€â”€ Redis INCR
â”œâ”€â”€ Compare-and-swap
â””â”€â”€ Lua scripts
```

### Clock Synchronization

```
Problem: Different servers, different clocks

Solution:
â”œâ”€â”€ Use single time source (Redis time)
â”œâ”€â”€ NTP synchronization
â””â”€â”€ Small tolerance window
```

### Hot Keys

```
Problem: Celebrity user overwhelms Redis

Solutions:
â”œâ”€â”€ Local cache with short TTL
â”œâ”€â”€ Probabilistic check
â””â”€â”€ Dedicated rate limit for hot users
```

---

## âœ… Key Takeaways

1. **Token bucket** is most versatile (allows bursts)
2. **Sliding window counter** is good balance
3. **Use Redis** for distributed rate limiting
4. **Include headers** to help clients
5. **Rate limit at API Gateway** for centralized control
6. **Different limits** for different endpoints/tiers
7. **Use atomic operations** to prevent race conditions
